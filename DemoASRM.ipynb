{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\quang\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\quang\\AppData\\Local\\Temp\\ipykernel_8144\\2158157440.py:23: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend('soundfile')\n"
     ]
    }
   ],
   "source": [
    "from tqdm import  tqdm\n",
    "# import wandb\n",
    "import torch\n",
    "# from torchsummary import summary\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "\n",
    "from transformers import (\n",
    "    Wav2Vec2Processor,\n",
    "    Wav2Vec2ForCTC,\n",
    "\n",
    "\n",
    ")\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "from copy import deepcopy\n",
    "# import jiwer\n",
    "from inference import Inference\n",
    "from datasets  import load_metric\n",
    "import IPython.display as ipd\n",
    "from MicRecorder import MicRecorder\n",
    "from multiprocessing import get_context\n",
    "torchaudio.set_audio_backend('soundfile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'checkpoint/checkpoint_Teacher'\n",
    "processor = Wav2Vec2Processor.from_pretrained('nguyenvulebinh/wav2vec2-base-vietnamese-250h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(model_name):\n",
    "    global inference\n",
    "    model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    inference=Inference(model,processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at checkpoint/checkpoint_Teacher were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_v', 'wav2vec2.encoder.pos_conv_embed.conv.weight_g']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at checkpoint/checkpoint_Teacher and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Unigrams not provided and cannot be automatically determined from LM file (only arpa format). Decoding accuracy might be reduced.\n",
      "No known unigrams provided, decoding results might be a lot worse.\n",
      "c:\\Users\\quang\\Desktop\\AllDemo_dev\\NLP\\Wav2vec2NST_ASR\\inference.py:26: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  self.wer_metric = load_metric(\"wer\")\n"
     ]
    }
   ],
   "source": [
    "createModel(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mr=MicRecorder()\n",
    "# mr.get_recording(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5007\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [07/Jan/2024 01:21:47] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2024 01:21:47] \"GET /static/index.css HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2024 01:21:47] \"GET /static/recorder.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2024 01:21:48] \"GET /static/Iconka-Saint-Whiskers-Cat-cupid-love.ico HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2024 01:22:01] \"GET /static/Iconka-Saint-Whiskers-Cat-cupid-love.ico HTTP/1.1\" 304 -\n",
      "c:\\Users\\quang\\Desktop\\AllDemo_dev\\NLP\\Wav2vec2NST_ASR\\inference.py:62: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
      "  pred= self.model(torch.tensor([specs]).to(device))\n",
      "127.0.0.1 - - [07/Jan/2024 01:22:04] \"POST /recorder HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [07/Jan/2024 01:22:12] \"POST /recorder HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, render_template,request,jsonify\n",
    "from werkzeug.utils import secure_filename\n",
    "app = Flask(__name__)\n",
    "app.config['JSON_AS_ASCII'] = False \n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return render_template('index.html')\n",
    "@app.route('/recorder', methods=['POST'])\n",
    "def micRecorder():\n",
    "    audio_file = request.files['audio']\n",
    "    filename = secure_filename(audio_file.filename)\n",
    "    audio_file.save( filename)\n",
    "    wave, sr = torchaudio.load('recording.wav')\n",
    "    predict=inference.predict(wave,sr,200)\n",
    "    return predict\n",
    "@app.route('/file', methods=['POST'])\n",
    "def fileRecorder():\n",
    "    audio_file = request.files['file']\n",
    "    filename = secure_filename(audio_file.filename)\n",
    "    audio_file.save( filename)\n",
    "    wave, sr = torchaudio.load('recording.wav')\n",
    "    predict=inference.predict(wave,sr,200)\n",
    "    return predict\n",
    "@app.route('/model',methods=['POST'])\n",
    "def updatamodel():\n",
    "    try:\n",
    "        model_name=request.get_json().get('data','')\n",
    "        createModel('checkpoint/'+model_name)\n",
    "        return jsonify({'message': 'Load model thÃ nh cÃ´ng',\"status\":'1'})\n",
    "    except :\n",
    "        return  jsonify({'message': 'KhÃ´ng load Ä‘Æ°á»£c model','status':'0'})\n",
    "if __name__ == '__main__':\n",
    "    app.run( port=5007)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
